{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 06: How to use synthetic data to enable unsupervised learning\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    " - AitiaExplorer synthetic data to enable unsupervised learning. \n",
    " - This achieved using a BayesianGaussianMixture (BGMM).\n",
    " - A BGMM can be used for clustering but it can also be used to model the data distribution that best represents the data.\n",
    " - This means that a BGMM can be used to model a data distribution and provide samples from that distribution, allowing the creation of sythentic data.\n",
    " - This synthetic data can then be combined with the real data, along with an extra label that separates the synthetic data from the real data.\n",
    " - This new dataset can then allow a classifier to be trained to recognise the real data in an unsupervised manner. \n",
    " - The code below in the method `get_synthetic_training_data` creates such a dataset.\n",
    " - These classifiers are used internally in AitiaExplorer to select the most important features in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import mixture\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from aitia_explorer.app import App\n",
    "\n",
    "# stop the warning clutter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Methods\n",
    "\n",
    "- `get_gmm_sample_data` creates sample data from a BGMM.\n",
    "- `get_synthetic_training_data` creates training data from synthetic and real data.\n",
    "- These methods are taken from the internals of AitiaExplorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmm_sample_data(incoming_df, column_list, sample_size):\n",
    "    \"\"\"\n",
    "    Unsupervised Learning in the form of BayesianGaussianMixture to create sample data.\n",
    "    \"\"\"\n",
    "    gmm = mixture.BayesianGaussianMixture(n_components=2,\n",
    "                                          covariance_type=\"full\",\n",
    "                                          n_init=100,\n",
    "                                          random_state=42).fit(incoming_df)\n",
    "    clustered_data = gmm.sample(sample_size)\n",
    "    clustered_df = pd.DataFrame(clustered_data[0], columns=column_list)\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_training_data(incoming_df):\n",
    "    \"\"\"\n",
    "    Creates synthetic training data by sampling from a BayesianGaussianMixture supplied distribution.\n",
    "    Synthetic data is then labelled differently from the original data.\n",
    "    \"\"\"\n",
    "    # number of records in df\n",
    "    number_records = len(incoming_df.index)\n",
    "\n",
    "    # get sample data from the unsupervised BayesianGaussianMixture\n",
    "    df_bgmm = get_gmm_sample_data(incoming_df, list(incoming_df), number_records)\n",
    "\n",
    "    # set the class on the samples\n",
    "    df_bgmm['original_data'] = 0\n",
    "\n",
    "    # add the class to a copy of incoming df, stops weird errors due to changed dataframes\n",
    "    working_df = incoming_df.copy(deep=True)\n",
    "    working_df['original_data'] = 1\n",
    "\n",
    "    # concatinate the two dataframes\n",
    "    df_combined = working_df.append(df_bgmm, ignore_index=True)\n",
    "\n",
    "    # shuffle the data\n",
    "    df_combined = df_combined.sample(frac=1)\n",
    "\n",
    "    # get the X and y\n",
    "    x = df_combined.drop(['original_data'], axis=1).values\n",
    "    y = df_combined['original_data'].values\n",
    "    y = y.ravel()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training \n",
    "\n",
    "- Now we will set up for training the classifiers by creating an AitiaExplorer instance and using it to load the [HEPAR II](https://www.bnlearn.com/bnrepository/#hepar2) dataset.\n",
    "- This data will then be divided into training and test datatsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aitia = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aitia.data.hepar2_10k_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ths synthetic data\n",
    "X, y = get_synthetic_training_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifiers\n",
    "\n",
    "- Now we will train some classifiers that normally need labelled data i.e. for supervised learning. \n",
    "- However, as we have created a synthetic training set, we can use these classifiers in an unsupervised manner to learn the real data.\n",
    "- The scores will then be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression, \n",
    "          SGDClassifier, \n",
    "          RandomForestClassifier, \n",
    "          GradientBoostingClassifier, \n",
    "          XGBClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56515475 0.47125954 0.42217683 ... 0.53574198 0.48767576 0.49978216]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-92860824a4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# store the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-92860824a4db>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "model_results = dict()\n",
    "\n",
    "# \n",
    "def sigmoid(y_pred):\n",
    "    y_return = []\n",
    "    for y in y_pred:\n",
    "        y_return.append(1 / (1 + math.exp(-y)))\n",
    "    return y_return\n",
    "\n",
    "for model in models:\n",
    "    current_model = model()\n",
    "    # fit the model\n",
    "    current_model.fit(X_train, y_train) \n",
    "    # predict\n",
    "    y_pred = current_model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    # store the accuracy\n",
    "    model_results[type(current_model).__name__] = accuracy_score(y_test, sigmoid(y_pred))\n",
    "\n",
    "model_df = pd.DataFrame(model_results)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Several of the classifiers have an almost perfect score on the synthetic dataset.\n",
    "- Even though the LinearClassifier does very poorly, it is still useful for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
