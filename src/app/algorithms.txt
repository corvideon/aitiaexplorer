------------------------------------------------------------------------
Algorithm ID:  skew
Description:  skew
This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses a skewness to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.
The Skew rule is differently motivated from the RSkew rule (see), though they both appeal to the skewness of the variables.
------------------------------------------------------------------------
Algorithm ID:  ts-imgs
Description:  ts-imgs
tsIMAGES is a version of tsGFCI which averages BIC scores across multiple data sets. Thus, it is used to search for a PAG (partial ancestral graph) from time series data from multiple units (subjects, countries, etc). tsIMAGES allows both for unmeasured (hidden, latent) variables and the possibility that different subjects have different causal parameters, though they share the same qualitative causal structure. As with IMAGES, the user can specify a “penalty score” to produce more sparse models. For the traditional definition of the BIC score, set the penalty to 1.0. See the documentation for IMAGES and tsGFCI.

It requires the score.

It accepts the prior knowledge.

It accepts multiple datasets.
------------------------------------------------------------------------
Algorithm ID:  multi-fask
Description:  multi-fask
Multi-FASK is a metascript that learns a model from a list of datasets in a method similar to IMaGES (see). For adjacencies, it uses FAS-Stable with the voting-based score from IMaGES used as a test (using all of the datasets, standardized), producing a single undirected graph G. It then orients each edge X--Y in G for each dataset using the FASK (see) left-right rule and orient X->Y if that rule orients X--Y as such in at least half of the datasets. The final graph is returned.
For FASK, See Sanchez-Romero R, Ramsey JD, Zhang K, Glymour MR, Huang B, Glymour C. Causal discovery of feedback networks with functional magnetic resonance imaging. Network Neuroscience 2018.

It accepts the prior knowledge.

It accepts multiple datasets.
------------------------------------------------------------------------
Algorithm ID:  rfci
Description:  rfci
A modification of the FCI algorithm in which some expensive steps are finessed and the output is somewhat differently interpreted. In most cases this runs faster than FCI (which can be slow in some steps) and is almost as informative. See Colombo et al., 2012.

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fask
Description:  fask
FASK learns a linear model in which all of the variables are skewed.
The idea is as follows. First, FAS-stable is run on the data, producing an undirected graph. We use the BIC score as a conditional independence test with a specified penalty discount c. This yields undirected graph G0 . The reason FAS-stable works for sparse cyclic models where the linear coefficients are all less than 1 is that correlations induced by long cyclic paths are statistically judged as zero, since they are products of multiple coefficients less than 1. Then, each of the X − Y adjacencies in G0 is oriented as a 2-cycle X += Y , or X → Y , or X ← Y . Taking up each adjacency in turn, one tests to see whether the adjacency is a 2-cycle by testing if the difference between corr(X, Y ) and corr(X, Y |X > 0), and corr(X, Y ) and corr(X, Y |Y > 0), are both significantly not zero. If so, the edges X → Y and X ← Y are added to the output graph G1 . If not, the Left-Right orientation is rule is applied: Orient X → Y in G1, if (E(X Y |X > 0)/ E(X 2|X > 0)E(Y 2 |X > 0) − E(X Y |Y > 0)/ E(X 2 |Y > 0)E(Y 2|Y > 0)) > 0; otherwise orient X ← Y . G1 will be a fully oriented graph. For some models, where the true coefficients of a 2-cycle between X and Y are more or less equal in magnitude but opposite in sign, FAS-stable may fail to detect an edge between X and Y when in fact a 2-cycle exists. In this case, we check explicitly whether corr(X, Y |X > 0) and corr(X, Y |Y > 0) differ by more than a set amount of 0.3. If so, the adjacency is added to the graph and oriented using the aforementioned rules.
See Sanchez-Romero R, Ramsey JD, Zhang K, Glymour MR, Huang B, Glymour C. Causal discovery of feedback networks with functional magnetic resonance imaging. Network Neuroscience 2018.

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  r-skew
Description:  r-skew
This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses a skewness to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, in particular in this case, if X and Y are skewed, the orientation is relatively straightforward. See Hyvarinen and Smith (2013) for details.
The RSkew rule is differently motivated from the Skew rule (see), though they both appeal to the skewness of the variables.
------------------------------------------------------------------------
Algorithm ID:  mgm
Description:  mgm
Need reference. Finds a Markov random field (with parents married) for a dataset in which continuous and discrete variables are mixed together. For example, if X->Y<-Z, the output will be X—Y—Z with X—Z. The parents of Y will be joined by an undirected edge, morally, even though this edge does not occur in the true model.
------------------------------------------------------------------------
Algorithm ID:  ftfc
Description:  ftfc
FTFC (Find Two Factor Clusters) is similar to FOFC, but instead of each cluster having one latent that is the parent of all of the measure in the cluster, it instead has two such latents. So each measure has two latent parents; these are two “factors.” Similarly to FOFC, constraints are checked for, but in this case, the constraints must be sextad constraints, and more of them must be satisfied for each pure cluster (see Kummerfelt et al., 2014). Thus, the number of measures in each cluster, once impure edges have been taken into account, must be at least six, preferably more.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  mbfs
Description:  mbfs
Markov blanket fan search. Similar to FGES-MB (see CCD-FGES, 2016) but using PC as the basic search instead of FGES. The rules of the PC search are restricted to just the variables in the Markov blanket of a target T, including T; the result is a graph that is a pattern over these variables.

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fofc
Description:  fofc
Searches for causal structure over latent variables, where the true models are Multiple Indicator Models (MIM’s) as described in the Graphs section. The idea is this. There is a set of latent (unmeasured) variables over which a directed acyclic model has been defined, Then for each of these latent L there are 3 (preferably 4) or more measures of that variable—that is, measured variables that are all children of L. Under these conditions, one may define tetrad constraints (see Spirtes et al., 2000). There is a theorem to the effect that if certain patterns of these tetrad constraints hold, there must be a latent common cause of all of them (the Tetrad Representation Theorem). The FOFC (Find One Factor Clusters) takes advantage of this fact. The basic idea is to build up clusters one at a time by adding variables that keep them pure in the sense that all relevant tetrad constraints still hold. There are different ways of going about this. One could try to build one cluster up as far as possible, then remove all of those variables from the set, and try to make a another cluster using the remaining variables (SAG, Seed and Grow). Or one can try in parallel to grow all possible clusters and then choose among the grown clusters using some criterion such as cluster size (GAP, Grow and Pick). In general, GAP is more accurate. The result is a clustering of variables. Once one has such a “measurement model, one can estimate (using the ESTIMATOR box) a covariance matrix over the latent variables that are parents of the measures and use some algorithm such as PC or GES to estimate a pattern over the latent variables. The algorithm to run PC or GES on this covariance matrix is called MimBuild (“MIM” is the graph, Multiple Indicator Model; “Build” means build). MimBUILD is an optional choice inside FOFC In this way, one may recover causal structure over the latents. The more measures one has for each latent the better the result is, generally. At least 3 measured indicator variables are needed for each latent variable. The larger the sample size the better. One important issue is that the algorithm is sensitive to so-called “impurities”—that is,causal edges among the measured variables, or between measured variables and multiple latent variables. The algorithm will in effect remove one measure in each impure pair from consideration.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fas
Description:  fas
This is just the adjacency search of the PC algorithm, included here for times when just the adjacency search is needed, as when one is subsequently just going to orient variables pairwise.

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  imgs_disc
Description:  imgs_disc
Adjusts the discrete BDeu variable score of FGES so allow for multiple datasets as input. The BDeu scores for each data set are averaged at each step of the algorithm, producing a model for all data sets that assumes they have the same graphical structure across dataset. Note that in order to use this algorithm in a nontrivial way, one needs to have loaded or simulated multiple dataset.

It accepts the prior knowledge.

It accepts multiple datasets.
------------------------------------------------------------------------
Algorithm ID:  glasso
Description:  glasso
A translation of the Fortran code for GLASSO (Graphical LASSO—see Friedman, Tibshirani anad Hastie, 2007) Like MGM, this produces an undirected graph in which parents are always married.
------------------------------------------------------------------------
Algorithm ID:  r3
Description:  r3
This is an algorithm that orients an edge X--Y for continuous variables based on non-Gaussian information. This rule in particular uses an entropy calculation to make the orientation. Note that if the variables X and Y are both Gaussian, and the model is linear, it is not possible to orient the edge X--Y pairwise; any attempt to do so would result in random orientation. But if X and Y are non-Gaussian, the orientation is fairly easy. This rule is similar to Hyvarinen and Smith's (2013) EB rule, but using Anderson Darling for the measure of non-Gaussianity, to somewhat better effect. See Ramsey et al. (2012).
------------------------------------------------------------------------
Algorithm ID:  ling
Description:  ling
Please add a description for ling.
------------------------------------------------------------------------
Algorithm ID:  gfci
Description:  gfci
GFCI is a combination of the FGES [CCD-FGES, 2016] algorithm and the FCI algorithm [Spirtes, 1993] that improves upon the accuracy and efficiency of FCI. In order to understand the basic methodology of GFCI, it is necessary to understand some basic facts about the FGES and FCI algorithms. The FGES algorithm is used to improve the accuracy of both the adjacency phase and the orientation phase of FCI by providing a more accurate initial graph that contains a subset of both the non-adjacencies and orientations of the final output of FCI. The initial set of nonadjacencies given by FGES is augmented by FCI performing a set of conditional independence tests that lead to the removal of some further adjacencies whenever a conditioning set is found that makes two adjacent variables independent. After the adjacency phase of FCI, some of the orientations of FGES are then used to provide an initial orientation of the undirected graph that is then augmented by the orientation phase of FCI to provide additional orientations. A verbose description of GFCI can be found here (discrete variables) and here (continuous variables).

It requires the independence test.

It requires the score.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fges
Description:  fges
FGES is an optimized and parallelized version of an algorithm developed by Meek [Meek, 1997] called the Greedy Equivalence Search (GES). The algorithm was further developed and studied by Chickering [Chickering, 2002]. GES is a Bayesian algorithm that heuristically searches the space of CBNs and returns the model with highest Bayesian score it finds. In particular, GES starts its search with the empty graph. It then performs a forward stepping search in which edges are added between nodes in order to increase the Bayesian score. This process continues until no single edge addition increases the score. Finally, it performs a backward stepping search that removes edges until no single edge removal can increase the score. More information is available here and here. The reference is Ramsey et al., 2017.
The algorithms requires a decomposable score—that is, a score that for the entire DAG model is a sum of logged scores of each variables given its parents in the model. The algorithms can take all continuous data (using the SEM BIC score), all discrete data (using the BDeu score) or a mixture of continuous and discrete data (using the Conditional Gaussian score); these are all decomposable scores.

It requires the score.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  rfci-bsc
Description:  rfci-bsc
RFCI-BSC is a combination of the RFCI [Colombo, 2012] algorithm and the Bayesian Scoring of Constraints (BSC) method [Jabbari, 2017] that can generate and probabilistically score multiple models, outputting the most probable one. This search algorithm is a hybrid method that derives a Bayesian probability that the set of independence tests associated with a given causal model are jointly correct. Using this constraint-based scoring method, we are able to score multiple causal models, which possibly contain latent variables, and output the most probable one. See [Jabbari, 2017].

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fci
Description:  fci
The FCI algorithm is a constraint-based algorithm that takes as input sample data and optional background knowledge and in the large sample limit outputs an equivalence class of CBNs that (including those with hidden confounders) that entail the set of conditional independence relations judged to hold in the population. It is limited to several thousand variables, and on realistic sample sizes it is inaccurate in both adjacencies and orientations. FCI has two phases: an adjacency phase and an orientation phase. The adjacency phase of the algorithm starts with a complete undirected graph and then performs a sequence of conditional independence tests that lead to the removal of an edge between any two adjacent variables that are judged to be independent, conditional on some subset of the observed variables; any conditioning set that leads to the removal of an adjacency is stored. After the adjacency phase, the resulting undirected graph has the correct set of adjacencies, but all of the edges are unoriented. FCI then enters an orientation phase that uses the stored conditioning sets that led to the removal of adjacencies to orient as many of the edges as possible. See [Spirtes, 1993].

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  pc-all
Description:  pc-all
PC algorithm (Spirtes and Glymour, Social Science Computer Review, 1991) is a pattern search which assumes that the underlying causal structure of the input data is acyclic, and that no two variables are caused by the same latent (unmeasured) variable. In addition, it is assumed that the input data set is either entirely continuous or entirely discrete; if the data set is continuous, it is assumed that the causal relation between any two variables is linear, and that the distribution of each variable is Normal. Finally, the sample should ideally be i.i.d.. Simulations show that PC and several of the other algorithms described here often succeed when these assumptions, needed to prove their correctness, do not strictly hold. The PC algorithm will sometimes output double headed edges. In the large sample limit, double headed edges in the output indicate that the adjacent variables have an unrecorded common cause, but PC tends to produce false positive double headed edges on small samples.
The PC algorithm is correct whenever decision procedures for independence and conditional independence are available. The procedure conducts a sequence of independence and conditional independence tests, and efficiently builds a pattern from the results of those tests. As implemented in TETRAD, PC is intended for multinomial and approximately Normal distributions with i.i.d. data. The tests have an alpha value for rejecting the null hypothesis, which is always a hypothesis of independence or conditional independence. For continuous variables, PC uses tests of zero correlation or zero partial correlation for independence or conditional independence respectively. For discrete or categorical variables, PC uses either a chi square or a g square test of independence or conditional independence (see Causation, Prediction, and Search for details on tests). In either case, the tests require an alpha value for rejecting the null hypothesis, which can be adjusted by the user. The procedures make no adjustment for multiple testing. (For PC, CPC, JPC, JCPC, FCI, all testing searches.)

It requires the independence test.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  lingam
Description:  lingam
LiNGAM (Shimizu et al., 2006) was one of the first of the algorithms that assumed linearity among the variables and non-Gaussianity of error term, and still one of the best for smaller models, for the basic algorithm, implemented here. The idea is to use the Independent Components Analysis (ICA) algorithm to check all permutations of the variables to find one that is a causal order—that is, one in which earlier variables can cause later variables but not vice-versa. The method is clever. First, since we assume the model is a directed acyclic graph (DAG), there must be some permutation of the variables for which the main diagonal of the inverse of the weight matrix contains no zeros. This gives us a permuted estimate of the weight matrix. Then we look for a permutation of this weight matrix that is lower triangular. There must be one, since the model is assumed to be a DAG. But a lower triangular weight matrix just gives a causal order, so we’re done.
In the referenced paper, we implement Algorithm A, which is described above. Once one has a causal order the only thing one needs to do is to eliminate the extra edges. For this, we use the causal order to define knowledge of tiers and run FGES.
Our implementation of LiNGAM has one parameter, penalty discount, used for the FGES adjacency search. The method as implemented does not scale much beyond 10 variables, because it is checking every permutation of all of the variables (twice). The implementation of ICA we use is FastIca (Hyvärinen et al., 2004).
Shimizu, S., Hoyer, P. O., Hyvärinen, A., & Kerminen, A. (2006). A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct), 2003-2030.
Hyvärinen, A., Karhunen, J., & Oja, E. (2004). Independent component analysis (Vol. 46). John Wiley & Sons.
------------------------------------------------------------------------
Algorithm ID:  imgs_cont
Description:  imgs_cont
Adjusts the continuous variable score (SEM BIC) of FGES so allow for multiple datasets as input. The linear, Gaussian BIC scores for each data set are averaged at each step of the algorithm, producing a model for all data sets that assumes they have the same graphical structure across dataset.

It accepts the prior knowledge.

It accepts multiple datasets.
------------------------------------------------------------------------
Algorithm ID:  ts-gfci
Description:  ts-gfci
tsGFCI uses a BIC score to search for a skeleton. Thus, the only user-specified parameter is an optional “penalty score” to bias the search in favor of more sparse models. See the description of the GES algorithm for discussion of the penalty score. For the traditional definition of the BIC score, set the penalty to 1.0. The orientation rules are the same as for FCI. As is the case with tsFCI, tsGFCI will automatically respect the time order of the variables and impose a repeating structure. Firstly, it puts lagged variables in appropriate tiers so, e.g., X3:2 can cause X3:1 and X3 but X3:1 cannot cause X3:2 and X3 cannot cause either X3:1 or X3:2. Also, it will assume that the causal structure is the same across time, so that if the edge between X1 and X2 is removed because this increases the BIC score, then also the edge between X1:1 and X2:1 is removed, and so on for additional lags if they exist. When some edge is removed as the result of a score increase, all similar (or “homologous”) edges are also removed.

It requires the independence test.

It requires the score.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  fges-mb
Description:  fges-mb
This is a restriction of the FGES algorithm to union of edges over the combined Markov blankets of a set of targets, including the targets. In the interface, just one target may be specified. See Ramsey et al., 2017 for details. In the general case, finding the graph over the Markov blanket variables of a target (including the target) is far faster than finding the pattern for all of the variables.

It requires the score.

It accepts the prior knowledge.
------------------------------------------------------------------------
Algorithm ID:  ts-fci
Description:  ts-fci
The tsFCI algorithm is a version of FCI for time series data. See the FCI documentation for a description of the FCI algorithm, which allows for unmeasured (hidden, latent) variables in the data-generating process and produces a PAG (partial ancestral graph). tsFCI takes as input a “time lag data set,” i.e., a data set which includes time series observations of variables X1, X2, X3, ..., and their lags X1:1, X2:1, X3:1, ..., X1:2, X2:2,X3:2, ... and so on. X1:n is the nth-lag of the variable X1. To create a time lag data set from a standard tabular data set (i.e., a matrix of observations of X1, X2, X3, ...), use the “create time lag data” function in the data manipulation toolbox. The user will be prompted to specify the number of lags (n), and a new data set will be created with the above naming convention. The new sample size will be the old sample size minus n.

It requires the independence test.

It accepts the prior knowledge.
